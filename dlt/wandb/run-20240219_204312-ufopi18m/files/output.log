
Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:03<00:00, 30.10it/s, loss=6.66, lr=9.4e-8, step=94]
I0219 20:43:19.223088 140215473209728 logging.py:61] Saving current state to logs/test/checkpoints/checkpoint-0/
W0219 20:43:19.224279 140215473209728 logging.py:61] Removed shared tensor {'embed_timestep.seq_pos_enc.pe'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
I0219 20:43:19.298579 140215473209728 logging.py:61] Model weights saved in logs/test/checkpoints/checkpoint-0/model.safetensors
I0219 20:43:19.413997 140215473209728 logging.py:61] Optimizer state saved in logs/test/checkpoints/checkpoint-0/optimizer.bin
I0219 20:43:19.414342 140215473209728 logging.py:61] Scheduler state saved in logs/test/checkpoints/checkpoint-0/scheduler.bin
I0219 20:43:19.414451 140215473209728 logging.py:61] Sampler state for dataloader 0 saved in logs/test/checkpoints/checkpoint-0/sampler.bin
I0219 20:43:19.414534 140215473209728 logging.py:61] Sampler state for dataloader 1 saved in logs/test/checkpoints/checkpoint-0/sampler_1.bin
I0219 20:43:19.416053 140215473209728 logging.py:61] Random states saved in logs/test/checkpoints/checkpoint-0/random_states_0.pkl
I0219 20:43:19.478174 140215473209728 cal_trainer.py:262] Saving checkpoint to logs/test/checkpoints/checkpoint-0/
Epoch 1:  10%|██████████▉                                                                                                       | 9/94 [00:00<00:03, 21.52it/s, loss=6.61, lr=1.03e-7, step=103]
############################################
<class 'odict_items'>
seq_pos_enc.pe torch.Size([5000, 1, 512])
seqTransEncoder.layers.0.self_attn.in_proj_weight torch.Size([1536, 512])
seqTransEncoder.layers.0.self_attn.in_proj_bias torch.Size([1536])
seqTransEncoder.layers.0.self_attn.out_proj.weight torch.Size([512, 512])
seqTransEncoder.layers.0.self_attn.out_proj.bias torch.Size([512])
seqTransEncoder.layers.0.linear1.weight torch.Size([1024, 512])
seqTransEncoder.layers.0.linear1.bias torch.Size([1024])
seqTransEncoder.layers.0.linear2.weight torch.Size([512, 1024])
seqTransEncoder.layers.0.linear2.bias torch.Size([512])
seqTransEncoder.layers.0.norm1.weight torch.Size([512])
seqTransEncoder.layers.0.norm1.bias torch.Size([512])
seqTransEncoder.layers.0.norm2.weight torch.Size([512])
seqTransEncoder.layers.0.norm2.bias torch.Size([512])
seqTransEncoder.layers.1.self_attn.in_proj_weight torch.Size([1536, 512])
seqTransEncoder.layers.1.self_attn.in_proj_bias torch.Size([1536])
seqTransEncoder.layers.1.self_attn.out_proj.weight torch.Size([512, 512])
seqTransEncoder.layers.1.self_attn.out_proj.bias torch.Size([512])
seqTransEncoder.layers.1.linear1.weight torch.Size([1024, 512])
seqTransEncoder.layers.1.linear1.bias torch.Size([1024])
seqTransEncoder.layers.1.linear2.weight torch.Size([512, 1024])
seqTransEncoder.layers.1.linear2.bias torch.Size([512])
seqTransEncoder.layers.1.norm1.weight torch.Size([512])
seqTransEncoder.layers.1.norm1.bias torch.Size([512])
seqTransEncoder.layers.1.norm2.weight torch.Size([512])
seqTransEncoder.layers.1.norm2.bias torch.Size([512])
seqTransEncoder.layers.2.self_attn.in_proj_weight torch.Size([1536, 512])
seqTransEncoder.layers.2.self_attn.in_proj_bias torch.Size([1536])
seqTransEncoder.layers.2.self_attn.out_proj.weight torch.Size([512, 512])
seqTransEncoder.layers.2.self_attn.out_proj.bias torch.Size([512])
seqTransEncoder.layers.2.linear1.weight torch.Size([1024, 512])
seqTransEncoder.layers.2.linear1.bias torch.Size([1024])
seqTransEncoder.layers.2.linear2.weight torch.Size([512, 1024])
seqTransEncoder.layers.2.linear2.bias torch.Size([512])
seqTransEncoder.layers.2.norm1.weight torch.Size([512])
seqTransEncoder.layers.2.norm1.bias torch.Size([512])
seqTransEncoder.layers.2.norm2.weight torch.Size([512])
seqTransEncoder.layers.2.norm2.bias torch.Size([512])
seqTransEncoder.layers.3.self_attn.in_proj_weight torch.Size([1536, 512])
seqTransEncoder.layers.3.self_attn.in_proj_bias torch.Size([1536])
seqTransEncoder.layers.3.self_attn.out_proj.weight torch.Size([512, 512])
seqTransEncoder.layers.3.self_attn.out_proj.bias torch.Size([512])
seqTransEncoder.layers.3.linear1.weight torch.Size([1024, 512])
seqTransEncoder.layers.3.linear1.bias torch.Size([1024])
seqTransEncoder.layers.3.linear2.weight torch.Size([512, 1024])
seqTransEncoder.layers.3.linear2.bias torch.Size([512])
seqTransEncoder.layers.3.norm1.weight torch.Size([512])
seqTransEncoder.layers.3.norm1.bias torch.Size([512])
seqTransEncoder.layers.3.norm2.weight torch.Size([512])
seqTransEncoder.layers.3.norm2.bias torch.Size([512])
seqTransEncoder.layers.4.self_attn.in_proj_weight torch.Size([1536, 512])
seqTransEncoder.layers.4.self_attn.in_proj_bias torch.Size([1536])
seqTransEncoder.layers.4.self_attn.out_proj.weight torch.Size([512, 512])
seqTransEncoder.layers.4.self_attn.out_proj.bias torch.Size([512])
seqTransEncoder.layers.4.linear1.weight torch.Size([1024, 512])
seqTransEncoder.layers.4.linear1.bias torch.Size([1024])
seqTransEncoder.layers.4.linear2.weight torch.Size([512, 1024])
seqTransEncoder.layers.4.linear2.bias torch.Size([512])
seqTransEncoder.layers.4.norm1.weight torch.Size([512])
seqTransEncoder.layers.4.norm1.bias torch.Size([512])
seqTransEncoder.layers.4.norm2.weight torch.Size([512])
seqTransEncoder.layers.4.norm2.bias torch.Size([512])
seqTransEncoder.layers.5.self_attn.in_proj_weight torch.Size([1536, 512])
seqTransEncoder.layers.5.self_attn.in_proj_bias torch.Size([1536])
seqTransEncoder.layers.5.self_attn.out_proj.weight torch.Size([512, 512])
seqTransEncoder.layers.5.self_attn.out_proj.bias torch.Size([512])
seqTransEncoder.layers.5.linear1.weight torch.Size([1024, 512])
seqTransEncoder.layers.5.linear1.bias torch.Size([1024])
seqTransEncoder.layers.5.linear2.weight torch.Size([512, 1024])
seqTransEncoder.layers.5.linear2.bias torch.Size([512])
seqTransEncoder.layers.5.norm1.weight torch.Size([512])
seqTransEncoder.layers.5.norm1.bias torch.Size([512])
seqTransEncoder.layers.5.norm2.weight torch.Size([512])
seqTransEncoder.layers.5.norm2.bias torch.Size([512])
seqTransEncoder.layers.6.self_attn.in_proj_weight torch.Size([1536, 512])
seqTransEncoder.layers.6.self_attn.in_proj_bias torch.Size([1536])
seqTransEncoder.layers.6.self_attn.out_proj.weight torch.Size([512, 512])
seqTransEncoder.layers.6.self_attn.out_proj.bias torch.Size([512])
seqTransEncoder.layers.6.linear1.weight torch.Size([1024, 512])
seqTransEncoder.layers.6.linear1.bias torch.Size([1024])
seqTransEncoder.layers.6.linear2.weight torch.Size([512, 1024])
seqTransEncoder.layers.6.linear2.bias torch.Size([512])
seqTransEncoder.layers.6.norm1.weight torch.Size([512])
seqTransEncoder.layers.6.norm1.bias torch.Size([512])
seqTransEncoder.layers.6.norm2.weight torch.Size([512])
seqTransEncoder.layers.6.norm2.bias torch.Size([512])
seqTransEncoder.layers.7.self_attn.in_proj_weight torch.Size([1536, 512])
seqTransEncoder.layers.7.self_attn.in_proj_bias torch.Size([1536])
seqTransEncoder.layers.7.self_attn.out_proj.weight torch.Size([512, 512])
seqTransEncoder.layers.7.self_attn.out_proj.bias torch.Size([512])
seqTransEncoder.layers.7.linear1.weight torch.Size([1024, 512])
seqTransEncoder.layers.7.linear1.bias torch.Size([1024])
seqTransEncoder.layers.7.linear2.weight torch.Size([512, 1024])
seqTransEncoder.layers.7.linear2.bias torch.Size([512])
seqTransEncoder.layers.7.norm1.weight torch.Size([512])
seqTransEncoder.layers.7.norm1.bias torch.Size([512])
seqTransEncoder.layers.7.norm2.weight torch.Size([512])
seqTransEncoder.layers.7.norm2.bias torch.Size([512])
embed_timestep.seq_pos_enc.pe torch.Size([5000, 1, 512])
embed_timestep.time_embed.0.weight torch.Size([512, 512])
embed_timestep.time_embed.0.bias torch.Size([512])
embed_timestep.time_embed.2.weight torch.Size([512, 512])
embed_timestep.time_embed.2.bias torch.Size([512])
output_process.0.weight torch.Size([4, 512])
output_process.0.bias torch.Size([4])
image_emb.0.weight torch.Size([256, 512])
image_emb.0.bias torch.Size([256])
xy_emb.0.weight torch.Size([112, 2])
xy_emb.0.bias torch.Size([112])
wh_emb.0.weight torch.Size([112, 2])
wh_emb.0.bias torch.Size([112])
r_emb.0.weight torch.Size([64, 1])
r_emb.0.bias torch.Size([64])
z_emb.0.weight torch.Size([64, 1])
z_emb.0.bias torch.Size([64])
ratio_emb.0.weight torch.Size([32, 1])
ratio_emb.0.bias torch.Size([32])
tokens_emb.0.weight torch.Size([512, 640])
tokens_emb.0.bias torch.Size([512])

Epoch 1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:02<00:00, 37.55it/s, loss=5.52, lr=1.88e-7, step=188]
Epoch 2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:02<00:00, 37.53it/s, loss=4.33, lr=2.82e-7, step=282]

Epoch 3:  59%|██████████████████████████████████████████████████████████████████                                               | 55/94 [00:21<00:01, 36.83it/s, loss=2.18, lr=3.37e-7, step=337]Traceback (most recent call last):
  File "/home/mineslab-ubuntu/KDH_CAL2/dlt/main.py", line 119, in <module>
    app.run(main)
  File "/home/mineslab-ubuntu/anaconda3/envs/DLT/lib/python3.9/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/mineslab-ubuntu/anaconda3/envs/DLT/lib/python3.9/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/mineslab-ubuntu/KDH_CAL2/dlt/main.py", line 94, in main
    TrainLoopCAL(accelerator=accelerator, model=model, diffusion=noise_scheduler,
  File "/home/mineslab-ubuntu/KDH_CAL2/dlt/trainers/cal_trainer.py", line 106, in train
    self.train_epoch_CAL(epoch)
  File "/home/mineslab-ubuntu/KDH_CAL2/dlt/trainers/cal_trainer.py", line 202, in train_epoch_CAL
    self.optimizer.step()
  File "/home/mineslab-ubuntu/anaconda3/envs/DLT/lib/python3.9/site-packages/accelerate/optimizer.py", line 145, in step
    self.optimizer.step(closure)
  File "/home/mineslab-ubuntu/anaconda3/envs/DLT/lib/python3.9/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/mineslab-ubuntu/anaconda3/envs/DLT/lib/python3.9/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/home/mineslab-ubuntu/anaconda3/envs/DLT/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/mineslab-ubuntu/anaconda3/envs/DLT/lib/python3.9/site-packages/torch/optim/adamw.py", line 162, in step
    adamw(params_with_grad,
  File "/home/mineslab-ubuntu/anaconda3/envs/DLT/lib/python3.9/site-packages/torch/optim/adamw.py", line 219, in adamw
    func(params,
  File "/home/mineslab-ubuntu/anaconda3/envs/DLT/lib/python3.9/site-packages/torch/optim/adamw.py", line 267, in _single_tensor_adamw
    step_t += 1
KeyboardInterrupt